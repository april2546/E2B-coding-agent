{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeb7f35",
   "metadata": {},
   "source": [
    "## About the Agent\n",
    "\n",
    "This is an agent that autonomously generates and executes code in a secure cloud sandbox to respond to user queries. It specifically works on data to perform statistical analysis and creates visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import load_env\n",
    "load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e41370",
   "metadata": {},
   "source": [
    "# Tool Calling\n",
    "\n",
    "- **Goal:** Create function schemas that tell the LLM what tools are available and how to call them\n",
    "- **Workflow:** Need something like execute_code for running Python, or write_file for creating files\n",
    "- **Info:** Tools are called by the LLM via function calling, so schemas must be precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69babef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Callable\n",
    "from e2b_code_interpreter import Execution, Sandbox\n",
    "\n",
    "# Tool Schema - tells the LLM what tools are available\n",
    "execute_code_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"execute_code\",\n",
    "    \"description\": \"Execute Python code and return the result or error.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"code\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Python code to execute as a string.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"code\"],\n",
    "        \"additionalProperties\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Tool Implementation - executes code in the sandbox\n",
    "def execute_code(sbx: Sandbox, code: str, language: str = \"python\") -> Execution:\n",
    "    execution = sbx.run_code(code, language)\n",
    "    metadata = {}\n",
    "    results = execution.results\n",
    "    for result in results:\n",
    "        if result.png:\n",
    "            metadata[\"images\"] = [result.png]\n",
    "            result.png = None\n",
    "            result.chart = None\n",
    "    return execution.to_json(), metadata\n",
    "\n",
    "# Helper to execute any tool by name\n",
    "def execute_tool(name: str, args: str, tools: dict[str, Callable], **kwargs):\n",
    "    metadata = {}\n",
    "    try:\n",
    "        args = json.loads(args)\n",
    "        if name not in tools:\n",
    "            return {\"error\": f\"Tool {name} doesn't exist.\"}\n",
    "        result, metadata = tools[name](**args, **kwargs)\n",
    "    except json.JSONDecodeError as e:\n",
    "        result = {\"error\": f\"{name} failed to parse arguments: {str(e)}\"}\n",
    "    except KeyError as e:\n",
    "        result = {\"error\": f\"Missing key in arguments: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        result = {\"error\": str(e)}\n",
    "    return result, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36edf2",
   "metadata": {},
   "source": [
    "# Agent Loop\n",
    "\n",
    "This section is the iterative loop that powers the coding agent. This is where the LLM reasons, calls tools, receives results, and decides what to do next.\n",
    "\n",
    "- **Goal:** Implement a multi-step agent that iterates until task completion or max steps\n",
    "- **Workflow:** Create a loop that alternates between LLM calls and tool execution\n",
    "- **Info:** Stop when LLM doesn't call any more functions and use max_steps to prevent infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b33c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import logging\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from typing import Generator, Literal, Optional, Callable\n",
    "from e2b_code_interpreter import Sandbox\n",
    "from IPython.display import Image, display\n",
    "from rich.logging import RichHandler\n",
    "from rich.panel import Panel\n",
    "from rich.console import Console\n",
    "\n",
    "# Logger setup\n",
    "LOG_LEVELS = {\n",
    "    \"DEBUG\": \"ðŸ›\", \"INFO\": \"âœ¨\", \"WARNING\": \"âš ï¸\",\n",
    "    \"ERROR\": \"âŒ\", \"CRITICAL\": \"ðŸ”¥\", \"TOOL\": \"ðŸ¤–\",\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(\"dlai\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False\n",
    "if not logger.handlers:\n",
    "    handler = RichHandler(rich_tracebacks=True, show_path=False, show_time=False, show_level=True, markup=False)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def log_tool_call(tool_name, args_str, result):\n",
    "    \"\"\"Logs a tool call with a card-like display.\"\"\"\n",
    "    MAX_DISPLAY_LENGTH = 200\n",
    "    result_str = str(result)\n",
    "    if len(args_str) > MAX_DISPLAY_LENGTH:\n",
    "        args_str = args_str[:MAX_DISPLAY_LENGTH] + \"...\"\n",
    "    if len(result_str) > MAX_DISPLAY_LENGTH:\n",
    "        result_str = result_str[:MAX_DISPLAY_LENGTH] + \"...\"\n",
    "    panel_content = f\"[bold]{tool_name}[/bold]\\n\"\n",
    "    panel_content += f\"Arguments: [cyan]{args_str}[/cyan]\\n\"\n",
    "    panel_content += f\"Result: [magenta]{result_str}[/magenta]\"\n",
    "    console.print(Panel(panel_content, title=f\"{LOG_LEVELS['TOOL']} Tool Call\", expand=False))\n",
    "\n",
    "# Compression prompt for long conversations\n",
    "SYSTEM_PROMPT_COMPRESS_MESSAGES = r\"\"\"You are the component that summarizes internal chat history into a given structure.\n",
    "When the conversation history grows too large, you will be invoked to distill the entire history into a concise, structured XML snapshot.\n",
    "\n",
    "<state_snapshot>\n",
    "    <overall_goal><!-- User's high-level objective --></overall_goal>\n",
    "    <key_knowledge><!-- Crucial facts and constraints --></key_knowledge>\n",
    "    <file_system_state><!-- Files created/modified/read --></file_system_state>\n",
    "    <recent_actions><!-- Last significant agent actions --></recent_actions>\n",
    "    <current_plan><!-- Step-by-step plan with status --></current_plan>\n",
    "</state_snapshot>\"\"\"\n",
    "\n",
    "TOKEN_LIMIT = 60_000\n",
    "COMPRESS_THRESHOLD = 0.7\n",
    "STATE_SNAPSHOT_PATTERN = re.compile(r\"<state_snapshot>(.*?)</state_snapshot>\", re.DOTALL)\n",
    "\n",
    "def clean_messages_for_llm(messages: list[dict]) -> list[dict]:\n",
    "    return [{k: v for k, v in msg.items() if not k.startswith(\"_\")} for msg in messages]\n",
    "\n",
    "def compress_messages(client: OpenAI, messages: list[dict]) -> list[dict]:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=[\n",
    "            {\"role\": \"developer\", \"content\": SYSTEM_PROMPT_COMPRESS_MESSAGES},\n",
    "            *messages,\n",
    "            {\"role\": \"user\", \"content\": \"First, reason in your scratchpad. Then, generate the <state_snapshot>.\"},\n",
    "        ],\n",
    "    )\n",
    "    text = response.output_text\n",
    "    context = \"\\n\".join(STATE_SNAPSHOT_PATTERN.findall(text))\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": f\"This is snapshot of the conversation so far:\\n{context}\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Got it. Thanks for the additional context!\"},\n",
    "    ]\n",
    "\n",
    "def get_compress_message_index(messages: list[dict]) -> int:\n",
    "    chars = [len(json.dumps(message)) for message in messages]\n",
    "    total_chars = sum(chars)\n",
    "    target_chars = total_chars * COMPRESS_THRESHOLD\n",
    "    curr_chars = 0\n",
    "    for index, char in enumerate(chars):\n",
    "        curr_chars += char\n",
    "        if curr_chars >= target_chars:\n",
    "            return index\n",
    "    return len(messages)\n",
    "\n",
    "def get_first_user_message_index(messages: list[dict]) -> int:\n",
    "    for index, message in enumerate(messages):\n",
    "        if \"role\" in message and message[\"role\"] == \"user\":\n",
    "            return index\n",
    "    return 0\n",
    "\n",
    "def maybe_compress_messages(client: OpenAI, messages: list[dict], usage: int) -> list[dict]:\n",
    "    if usage <= TOKEN_LIMIT * COMPRESS_THRESHOLD:\n",
    "        return messages\n",
    "    compress_index = get_compress_message_index(messages)\n",
    "    if compress_index >= len(messages):\n",
    "        return messages\n",
    "    compress_index += get_first_user_message_index(messages[compress_index:])\n",
    "    if compress_index <= 0:\n",
    "        return messages\n",
    "    last_message = messages[compress_index - 1]\n",
    "    if \"type\" in last_message and last_message[\"type\"] == \"function_call\":\n",
    "        compress_index += 1\n",
    "    to_compress_messages = messages[:compress_index]\n",
    "    to_keep_messages = messages[compress_index:]\n",
    "    if len(to_compress_messages) > 0:\n",
    "        logger.info(f\"[agent] compressing messages [0...{compress_index}]...\")\n",
    "        return [*compress_messages(client, to_compress_messages), *to_keep_messages]\n",
    "    return messages\n",
    "\n",
    "def coding_agent(\n",
    "    client: OpenAI,\n",
    "    sbx: Sandbox,\n",
    "    query: str,\n",
    "    tools: dict[str, Callable],\n",
    "    tools_schemas: list[dict],\n",
    "    max_steps: int = 5,\n",
    "    system: Optional[str] = \"You are a senior python programmer\",\n",
    "    messages: Optional[list[dict]] = None,\n",
    "    usage: Optional[int] = 0,\n",
    "    model: Literal[\"gpt-4.1-mini\", \"gpt-5-mini\"] = \"gpt-4.1-mini\",\n",
    "    **model_kwargs,\n",
    ") -> Generator[tuple[dict, dict, int], None, tuple[list[dict], int]]:\n",
    "    \"\"\"Main agent loop that iterates between LLM calls and tool execution.\"\"\"\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "    \n",
    "    user_message = {\"role\": \"user\", \"content\": query}\n",
    "    messages.append(user_message)\n",
    "    yield user_message, messages, usage\n",
    "\n",
    "    steps = 0\n",
    "    while steps < max_steps:\n",
    "        messages = maybe_compress_messages(client, clean_messages_for_llm(messages), usage)\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            input=[\n",
    "                {\"role\": \"developer\", \"content\": system},\n",
    "                *clean_messages_for_llm(messages),\n",
    "            ],\n",
    "            tools=tools_schemas,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        usage = response.usage.total_tokens\n",
    "        has_function_call = False\n",
    "        \n",
    "        for part in response.output:\n",
    "            messages.append(part.to_dict())\n",
    "            yield part.to_dict(), messages, usage\n",
    "            if part.type == \"function_call\":\n",
    "                has_function_call = True\n",
    "                name = part.name\n",
    "                result, metadata = execute_tool(name, part.arguments, tools, sbx=sbx)\n",
    "                result_msg = {\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": part.call_id,\n",
    "                    \"output\": json.dumps(result),\n",
    "                    \"_metadata\": metadata,\n",
    "                }\n",
    "                messages.append(result_msg)\n",
    "                yield result_msg, messages, usage\n",
    "\n",
    "        steps += 1\n",
    "        if not has_function_call:\n",
    "            break\n",
    "\n",
    "    return messages, usage\n",
    "\n",
    "def log(generator_func, *args, **kwargs):\n",
    "    \"\"\"Wraps the coding_agent and handles logging.\"\"\"\n",
    "    gen = generator_func(*args, **kwargs)\n",
    "    step = 0\n",
    "    pending_tool_calls = {}\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            part_dict, messages, usage = next(gen)\n",
    "            part_type = part_dict.get(\"type\")\n",
    "\n",
    "            if part_type == \"reasoning\":\n",
    "                if step == 0:\n",
    "                    logger.info(f\"[agent-#{step}] Thinking...\")\n",
    "                    step += 1\n",
    "                logger.info(\" ...\")\n",
    "            elif part_type == \"message\":\n",
    "                content = part_dict.get(\"content\")\n",
    "                if content and content[0].get(\"text\"):\n",
    "                    logger.info(f\"{content[0]['text']}\")\n",
    "            elif part_type == \"function_call\":\n",
    "                call_id = part_dict.get(\"call_id\")\n",
    "                name = part_dict.get(\"name\")\n",
    "                arguments = part_dict.get(\"arguments\")\n",
    "                pending_tool_calls[call_id] = (name, arguments)\n",
    "            elif part_type == \"function_call_output\":\n",
    "                call_id = part_dict.get(\"call_id\")\n",
    "                if call_id in pending_tool_calls:\n",
    "                    name, arguments = pending_tool_calls.pop(call_id)\n",
    "                    result = json.loads(part_dict.get(\"output\", \"{}\"))\n",
    "                    log_tool_call(name, arguments, result)\n",
    "                metadata = part_dict.get(\"_metadata\")\n",
    "                if metadata:\n",
    "                    images = metadata.get(\"images\")\n",
    "                    if images:\n",
    "                        for image in images:\n",
    "                            display(Image(data=base64.b64decode(image)))\n",
    "\n",
    "    except StopIteration as e:\n",
    "        messages, final_usage = e.value\n",
    "        logger.info(f\"[agent] tokens: {final_usage} total\")\n",
    "        return messages, final_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeee51c",
   "metadata": {},
   "source": [
    "# Sandbox Execution\n",
    "\n",
    "Use the agent in the cloud through a sandbox. Instead of running code locally, execute everything in an E2B sandbox which is a secure, isolated environment for untrusted code.\n",
    "\n",
    "- **Goal:** Integrate E2B sandbox with the coding agent for safe cloud execution\n",
    "- **Workflow:** Create sandbox, modify execute_code to use sandbox, update agent to pass sandbox to tools\n",
    "- **Remember:** Sandboxes are persistent, so you can reconnect, query by metadata, and serve websites from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb4c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox, SandboxQuery, SandboxState\n",
    "from uuid import uuid4\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_FILENAME = \"sbx.cache\"\n",
    "\n",
    "def create_sandbox(template: str = None, overwrite: bool = False, **kwargs) -> Sandbox:\n",
    "    \"\"\"Create or reconnect to a sandbox.\"\"\"\n",
    "    cache_file = Path(CACHE_FILENAME)\n",
    "\n",
    "    if cache_file.exists():\n",
    "        name = cache_file.read_text()\n",
    "    else:\n",
    "        name = f\"dlai-sbx-{template}-{uuid4()}\"\n",
    "        cache_file.write_text(name)\n",
    "\n",
    "    if not overwrite:\n",
    "        running_sandboxes = Sandbox.list(\n",
    "            SandboxQuery(metadata={\"name\": name}, state=[SandboxState.RUNNING])\n",
    "        ).next_items()\n",
    "        if running_sandboxes:\n",
    "            sandbox = Sandbox.connect(running_sandboxes[0].sandbox_id)\n",
    "            logger.info(f\"[sandbox] Reconnecting to Sandbox(id={sandbox.sandbox_id})\")\n",
    "            return sandbox\n",
    "\n",
    "    sandbox = Sandbox.create(timeout=60 * 60, metadata={\"name\": name}, template=template, **kwargs)\n",
    "    logger.info(f\"[sandbox] Creating new Sandbox(id={sandbox.sandbox_id})\")\n",
    "    return sandbox\n",
    "\n",
    "# Create the sandbox\n",
    "sbx = create_sandbox()\n",
    "\n",
    "# Upload the data file to the sandbox\n",
    "with open(\"pokemon.csv\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "sbx.files.write(\"data.csv\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a5af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "system = \"\"\"You are a senior python programmer. \n",
    "You must run the code using the `execute_code` tool.\n",
    "The user has uploaded a data.csv.\n",
    "You help the user understanding the data \n",
    "by creating interesting plots.\n",
    "\"\"\"\n",
    "\n",
    "# Map tool names to implementations\n",
    "tools = {\"execute_code\": execute_code}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "query = \"What is the data about?\"\n",
    "\n",
    "messages, usage = log(coding_agent,\n",
    "    messages=messages,\n",
    "    query=query,\n",
    "    client=client,\n",
    "    system=system,\n",
    "    tools_schemas=[execute_code_schema],\n",
    "    tools=tools,\n",
    "    max_steps=10,\n",
    "    sbx=sbx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0648e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you aggregate the pokemons by type?\"\n",
    "\n",
    "messages, usage = log(coding_agent,\n",
    "    messages=messages,\n",
    "    query=query,\n",
    "    client=client,\n",
    "    system=system,\n",
    "    tools_schemas=[execute_code_schema],\n",
    "    tools=tools,\n",
    "    max_steps=10,\n",
    "    sbx=sbx,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e033f5",
   "metadata": {},
   "source": [
    "# Gradio UI\n",
    "\n",
    "Below is the Gradio UI to have a nicer user experience chatting with the code generation agent. It has some built in features to provide information such as context stack, number of tokens used in the result, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.ui import ui\n",
    "\n",
    "ui(coding_agent,\n",
    "    messages,\n",
    "    client=client,\n",
    "    system=system,\n",
    "    tools_schemas=[execute_code_schema],\n",
    "    tools=tools,\n",
    "    max_steps=10,\n",
    "    sbx=sbx,\n",
    ").launch(share=True, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c21de",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "- [E2B Documentation](https://e2b.dev/docs)\n",
    "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [E2B GitHub](https://github.com/e2b-dev/e2b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
